# -*- coding: utf-8 -*-
"""
Data Manager - çµ±ä¸€è³‡æ–™ç®¡ç†ç³»çµ±
é›†ä¸­ç®¡ç†æ‰€æœ‰ TXF1_Minute è³‡æ–™çš„è¼‰å…¥å’Œå¿«å–
"""

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
import warnings
from typing import Optional, Dict, Any
import threading

warnings.filterwarnings('ignore')

class DataManager:
    """çµ±ä¸€è³‡æ–™ç®¡ç†å™¨"""
    
    def __init__(self):
        self._data_cache = {}
        self._cache_lock = threading.Lock()
        self._data_info = {}
        
        # é è¨­è³‡æ–™æª”æ¡ˆè·¯å¾‘
        self.default_data_paths = [
            "data/TXF1_Minute_2020-01-01_2025-07-04.txt",  # ä¸»è¦è³‡æ–™ç›®éŒ„
            "multi_strategy_system/TXF1_Minute_2020-01-01_2025-07-04.txt",  # ç³»çµ±ç›®éŒ„
            "data/TXF1_Minute_2020-01-01_2025-06-16.txt",  # èˆŠç‰ˆæœ¬å‚™ç”¨
            "TXF1_Minute_2020-01-01_2025-07-04.txt",  # ç•¶å‰ç›®éŒ„
            "TXF1_Minute_2020-01-01_2025-06-16.txt"  # èˆŠç‰ˆæœ¬ç•¶å‰ç›®éŒ„
        ]
        
        # è³‡æ–™æª”æ¡ˆè³‡è¨Š
        self.data_files_info = {
            "TXF1_Minute_2020-01-01_2025-07-04.txt": {
                "description": "æœ€æ–°ç‰ˆæœ¬ TXF1 åˆ†é˜è³‡æ–™ (2020-2025)",
                "size_mb": 108,
                "rows": "ç´„ 2.5M ç­†",
                "preferred": True
            },
            "TXF1_Minute_2020-01-01_2025-06-16.txt": {
                "description": "èˆŠç‰ˆæœ¬ TXF1 åˆ†é˜è³‡æ–™ (2020-2025)",
                "size_mb": 100,
                "rows": "ç´„ 2.3M ç­†", 
                "preferred": False
            }
        }
    
    def find_data_file(self, filename: Optional[str] = None) -> str:
        """
        å°‹æ‰¾è³‡æ–™æª”æ¡ˆ
        
        Args:
            filename: æŒ‡å®šçš„æª”æ¡ˆåç¨±ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•å°‹æ‰¾
            
        Returns:
            str: æ‰¾åˆ°çš„æª”æ¡ˆå®Œæ•´è·¯å¾‘
        """
        if filename:
            # æª¢æŸ¥æŒ‡å®šæª”æ¡ˆæ˜¯å¦å­˜åœ¨
            for base_path in ["", "data/", "multi_strategy_system/"]:
                full_path = os.path.join(base_path, filename)
                if os.path.exists(full_path):
                    return full_path
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æŒ‡å®šçš„è³‡æ–™æª”æ¡ˆ: {filename}")
        
        # è‡ªå‹•å°‹æ‰¾æœ€ä½³è³‡æ–™æª”æ¡ˆ
        for path in self.default_data_paths:
            if os.path.exists(path):
                print(f"ğŸ“ æ‰¾åˆ°è³‡æ–™æª”æ¡ˆ: {path}")
                return path
        
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå˜—è©¦æœå°‹æ•´å€‹ç›®éŒ„
        for root, dirs, files in os.walk("."):
            for file in files:
                if file.startswith("TXF1_Minute") and file.endswith(".txt"):
                    full_path = os.path.join(root, file)
                    print(f"ğŸ” æœå°‹åˆ°è³‡æ–™æª”æ¡ˆ: {full_path}")
                    return full_path
        
        raise FileNotFoundError("æ‰¾ä¸åˆ°ä»»ä½• TXF1_Minute è³‡æ–™æª”æ¡ˆ")
    
    def load_data(self, 
                  filename: Optional[str] = None,
                  use_cache: bool = True,
                  force_reload: bool = False,
                  **kwargs) -> pd.DataFrame:
        """
        è¼‰å…¥è³‡æ–™ï¼Œæ”¯æ´å¿«å–æ©Ÿåˆ¶
        
        Args:
            filename: è³‡æ–™æª”æ¡ˆåç¨±
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–
            force_reload: å¼·åˆ¶é‡æ–°è¼‰å…¥
            **kwargs: å‚³éçµ¦ pd.read_csv çš„åƒæ•¸
            
        Returns:
            pd.DataFrame: è¼‰å…¥çš„è³‡æ–™
        """
        # å°‹æ‰¾æª”æ¡ˆ
        file_path = self.find_data_file(filename)
        
        # æª¢æŸ¥å¿«å–
        cache_key = f"{file_path}_{hash(str(kwargs))}"
        
        if use_cache and not force_reload:
            with self._cache_lock:
                if cache_key in self._data_cache:
                    print(f"ğŸ“‹ ä½¿ç”¨å¿«å–è³‡æ–™: {file_path}")
                    return self._data_cache[cache_key].copy()
        
        # è¼‰å…¥è³‡æ–™
        print(f"ğŸ“¥ è¼‰å…¥è³‡æ–™æª”æ¡ˆ: {file_path}")
        start_time = datetime.now()
        
        try:
            # é è¨­åƒæ•¸
            default_params = {
                'sep': '\t',
                'parse_dates': ['datetime'],
                'index_col': 'datetime',
                'engine': 'python'
            }
            
            # åˆä½µåƒæ•¸
            load_params = {**default_params, **kwargs}
            
            # è¼‰å…¥è³‡æ–™
            df = pd.read_csv(file_path, **load_params)
            
            # è³‡æ–™æ¸…ç†
            df = self._clean_data(df)
            
            # å„²å­˜åˆ°å¿«å–
            if use_cache:
                with self._cache_lock:
                    self._data_cache[cache_key] = df.copy()
                    self._data_info[cache_key] = {
                        'file_path': file_path,
                        'load_time': datetime.now(),
                        'rows': len(df),
                        'columns': list(df.columns),
                        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024
                    }
            
            load_time = (datetime.now() - start_time).total_seconds()
            print(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆ: {len(df):,} ç­†è¨˜éŒ„, è€—æ™‚ {load_time:.2f} ç§’")
            
            return df
            
        except Exception as e:
            print(f"âŒ è³‡æ–™è¼‰å…¥å¤±æ•—: {str(e)}")
            raise
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…ç†è³‡æ–™
        
        Args:
            df: åŸå§‹è³‡æ–™
            
        Returns:
            pd.DataFrame: æ¸…ç†å¾Œçš„è³‡æ–™
        """
        # ç§»é™¤é‡è¤‡è³‡æ–™
        original_len = len(df)
        df = df.drop_duplicates()
        if len(df) < original_len:
            print(f"ğŸ§¹ ç§»é™¤ {original_len - len(df)} ç­†é‡è¤‡è³‡æ–™")
        
        # ç¢ºä¿ç´¢å¼•æ’åº
        df = df.sort_index()
        
        # ç§»é™¤ç„¡æ•ˆè³‡æ–™
        df = df.dropna()
        
        # ç¢ºä¿æ•¸å€¼æ¬„ä½ç‚ºæ•¸å€¼å‹åˆ¥
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç§»é™¤ç•°å¸¸å€¼
        for col in ['open', 'high', 'low', 'close']:
            if col in df.columns:
                # ç§»é™¤åƒ¹æ ¼ç‚º 0 æˆ–è² æ•¸çš„è³‡æ–™
                df = df[df[col] > 0]
        
        return df
    
    def get_cached_data_info(self) -> Dict[str, Any]:
        """
        å–å¾—å¿«å–è³‡æ–™è³‡è¨Š
        
        Returns:
            Dict: å¿«å–è³‡æ–™è³‡è¨Š
        """
        with self._cache_lock:
            return self._data_info.copy()
    
    def clear_cache(self, cache_key: Optional[str] = None):
        """
        æ¸…é™¤å¿«å–
        
        Args:
            cache_key: ç‰¹å®šçš„å¿«å–éµï¼Œå¦‚æœç‚º None å‰‡æ¸…é™¤æ‰€æœ‰å¿«å–
        """
        with self._cache_lock:
            if cache_key:
                if cache_key in self._data_cache:
                    del self._data_cache[cache_key]
                    del self._data_info[cache_key]
                    print(f"ğŸ—‘ï¸ æ¸…é™¤å¿«å–: {cache_key}")
            else:
                self._data_cache.clear()
                self._data_info.clear()
                print("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å¿«å–")
    
    def get_cache_status(self) -> Dict[str, Any]:
        """
        å–å¾—å¿«å–ç‹€æ…‹
        
        Returns:
            Dict: å¿«å–ç‹€æ…‹è³‡è¨Š
        """
        with self._cache_lock:
            total_memory = sum(info.get('memory_usage_mb', 0) for info in self._data_info.values())
            return {
                'cache_count': len(self._data_cache),
                'total_memory_mb': total_memory,
                'cached_files': list(self._data_info.keys())
            }
    
    def export_data_info(self, output_file: str = "data_info.json"):
        """
        åŒ¯å‡ºè³‡æ–™è³‡è¨Š
        
        Args:
            output_file: è¼¸å‡ºæª”æ¡ˆåç¨±
        """
        info = {
            'timestamp': datetime.now().isoformat(),
            'cache_status': self.get_cache_status(),
            'cached_data_info': self.get_cached_data_info(),
            'available_files': {}
        }
        
        # æª¢æŸ¥å¯ç”¨çš„è³‡æ–™æª”æ¡ˆ
        for filename in self.data_files_info.keys():
            for base_path in ["", "data/", "multi_strategy_system/"]:
                full_path = os.path.join(base_path, filename)
                if os.path.exists(full_path):
                    file_size = os.path.getsize(full_path) / 1024 / 1024  # MB
                    info['available_files'][full_path] = {
                        'size_mb': file_size,
                        'exists': True,
                        'info': self.data_files_info[filename]
                    }
                    break
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è³‡æ–™è³‡è¨Šå·²åŒ¯å‡ºè‡³: {output_file}")

# å…¨åŸŸè³‡æ–™ç®¡ç†å™¨å¯¦ä¾‹
data_manager = DataManager()

# ä¾¿åˆ©å‡½æ•¸
def load_txf_data(filename: Optional[str] = None, **kwargs) -> pd.DataFrame:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šè¼‰å…¥ TXF è³‡æ–™
    
    Args:
        filename: æª”æ¡ˆåç¨±
        **kwargs: å…¶ä»–åƒæ•¸
        
    Returns:
        pd.DataFrame: è¼‰å…¥çš„è³‡æ–™
    """
    return data_manager.load_data(filename, **kwargs)

def get_data_info() -> Dict[str, Any]:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå–å¾—è³‡æ–™è³‡è¨Š
    
    Returns:
        Dict: è³‡æ–™è³‡è¨Š
    """
    return data_manager.get_cache_status()

if __name__ == "__main__":
    # æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨
    print("ğŸ§ª æ¸¬è©¦è³‡æ–™ç®¡ç†å™¨...")
    
    try:
        # è¼‰å…¥è³‡æ–™
        df = load_txf_data()
        print(f"ğŸ“Š è¼‰å…¥æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
        print(f"ğŸ“… è³‡æ–™æœŸé–“: {df.index.min()} åˆ° {df.index.max()}")
        print(f"ğŸ“ˆ æ¬„ä½: {list(df.columns)}")
        
        # é¡¯ç¤ºå¿«å–ç‹€æ…‹
        cache_status = get_data_info()
        print(f"ğŸ’¾ å¿«å–ç‹€æ…‹: {cache_status}")
        
        # åŒ¯å‡ºè³‡æ–™è³‡è¨Š
        data_manager.export_data_info()
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {str(e)}") 